# PAT -- Paradigm association tool

The PAT tool is useful for producing ranked (paradigm, lemma) pairs of candidates for an input list of OOVs and an existing Apertium dix. The tool can not suggest new paradigms, just associations of OOVs to already existing paradigms.

There are three main steps necessary to produce the ranked output:

1. producing a frequency list of tokens from a corpus / Wikipedia dump
2. generating candidates of (paradigm, lemma) pairs from a list of OOVs
3. ranking the candidates by using the corpus information

These three steps are described in the following three subsections.

## Resources

To be able to rank the (paradigm, lemma) candidates for an OOV, we need corpus frequency information. This information can be obtained through various sources. If you do not have a frequency list like the Italian example file available from [Resources/Wikipedia/ranking.ita](Resources/Wikipedia/ranking.ita), you can find instructions on how to obtain one from a Wikipedia dump on [Resources/Wikipedia/](Resources/Wikipedia/).

## BuildDataset

Tool for building the datasets needed for training/testing the Ranker. This tool takes an Apertium dictionary in format .dix, expands all the entries in the dictionary, and detects, for each of the surface forms generated, which candidate pairs stem/paradigm could describe the surface form. The entries used to build the datasets can be filtered by defining a collection of valid lexical categories.

Two datasets can be craeated with this tool: a training set and a test set, both with the same structure. The size of each dataset is defined by setting the ratio of entries that will be used to build the training set (if the ratio is 1.0 no test set will be created). 

As mentioned above, each dataset consists of a list of JSON objects (one per line) that represent an evaluation instance. Every evaluation instance contains:
 - the surface form of the word being evaluated;
 - the gold candidate (the pair stem/paradigm defined in the dictionary); and
 - all the possible stem/paradigm candidates in the dictionary that could describe the word.

Every candidate in the JSON file consists of:
 - a stem;
 - a paradigm;
 - a lema (the representative form for this pair stem/lemma); 
 - a list of surface forms that could be generated by combining the stem and the lemma; every surface form is provided togheter with the collection of lexical information (labels) in the corresponding paradigm.

To tool is run as follows:
```
java -jar BuildDataset.jar -p <TRAINING_RATIO> -c <WORD_FREQUENCY_IN_CORPUS> -d <DIX_DICTIONARY> -v <LIST_OF_LEXICAL_CATEGORIES> --training-output <OUTPUT_FILE_FOR_TRAINING_SET> --test-output <OUTPUT_FILE_FOR_TRAINING_SET>
```
Where options correspond to:
 - ```-p``` a value between 0 and 1 that defines the ratio of the entries to be used to build the training set (if the value is set to 0, no test set is generated)
 - ```-c``` a file containing a list of word frequences such those provided in the directory Resources
 - ```-d``` Apertium dictionary in format .dix
 - ```-v``` Comma-separated list of valid lexical categories
 - ```--training-output``` File where the training dataset will be writen
 - ```--test-output``` File where the test dataset will be writen

## DictionaryAnalyser

Miquel's tool for generating candidates from a list of OOVs. A script for splitting into train and test for experiments is available as well.

## Ranker

For now the ranking is performed by following the simple heuristic from (Espl√†-Gomis et al. 2011).

This ranker combines the first step output (corpus frequency calculation, [Resources/Wikipedia](Resources/Wikipedia)) and the second step output (candidate generation, [BuildDataset/](BuildDataset/)) of the tool.

An example run of the tool, if generated candidates for Italian can be found on ```BuildDataset/example_ita_candidates.gz``` and the Italian Wikipedia token frequencies on ```Resources/Wikipedia/ranking.ita```, is this:

```
$ python Ranker/rank.py BuildDataset/example_ita_candidates.gz Resources/Wikipedia/ranking.ita Ranker/example_ita_candidates.out.gz
Corpus loaded
Processed 10
Processed 20
Processed 30
Processed 40
Processed 50
Processed 60
Processed 70
Processed 80
Processed 90
Processed 100
Starting writing output
Finished writing output
Mean reciprocal rank (MRR) on the given dataset is 0.526463052849
The distribution of the first position with a correct candidate is [(1, 48), (3, 4), (4, 2), (8, 8), (12, 1), (17, 6), (19, 17), (23, 6), (26, 1), (27, 1), (31, 1), (35, 1), (41, 2), (47, 1), (68, 1)]
```

The output of the ranker can be observed in the output file ```Ranker/example_ita_candidates.out.gz```. It is a gzipped json file which can be loaded in the Apertium paradigm association frontend.
